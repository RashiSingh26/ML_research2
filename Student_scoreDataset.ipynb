{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"initial_id","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Bidirectional, Input\n\ndata = {\n    \"Hours\": [1.2, 2.0, 3.5, 4.0, 5.3, 6.1, 7.2, 8.0, 9.5],\n    \"Scores\": [18, 28, 33, 42, 57, 63, 77, 85, 97]\n}\n\ndf = pd.DataFrame(data)\nprint(\"Sample Data:\\n\", df.head())\n\nX = df[['Hours']].values\ny = df['Scores'].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.35, random_state=123)\n\nmodels = {\n    \"Linear Regression\": LinearRegression(),\n    \"SVM\": SVR(kernel='rbf', C=1.2, epsilon=0.2),\n    \"Random Forest\": RandomForestRegressor(n_estimators=120, max_depth=5, random_state=123),\n    \"XGBoost\": XGBRegressor(objective='reg:squarederror', n_estimators=120, learning_rate=0.12, random_state=123),\n    \"KNN\": KNeighborsRegressor(n_neighbors=4),\n}\n\nresults = {}\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    results[name] = {\"MSE\": mean_squared_error(y_test, preds), \"R2\": r2_score(y_test, preds)}\n\n#Naive Bayes\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nnb = GaussianNB()\ny_train_int = np.round(y_train)\nnb.fit(X_train_scaled, y_train_int)\npreds = nb.predict(X_test_scaled)\nresults[\"Naive Bayes (approx)\"] = {\"MSE\": mean_squared_error(y_test, preds), \"R2\": r2_score(y_test, preds)}\n\nmlp = MLPRegressor(hidden_layer_sizes=(70,30), max_iter=1800, random_state=123)\nmlp.fit(X_train, y_train)\npreds = mlp.predict(X_test)\nresults[\"MLP (Sklearn)\"] = {\"MSE\": mean_squared_error(y_test, preds), \"R2\": r2_score(y_test, preds)}\n\n#Deep Learning Model\nX_train_rnn = np.expand_dims(X_train, axis=1)\nX_test_rnn = np.expand_dims(X_test, axis=1)\n\ndef build_and_train(model, name):\n    model.compile(optimizer='adam', loss='mse')\n    model.fit(X_train_rnn, y_train, epochs=120, verbose=0)\n    preds = model.predict(X_test_rnn).flatten()\n    results[name] = {\"MSE\": mean_squared_error(y_test, preds), \"R2\": r2_score(y_test, preds)}\n\nrnn = Sequential([Input(shape=(1,1)), SimpleRNN(28), Dense(1)])\nbuild_and_train(rnn, \"RNN\")\n\nlstm = Sequential([Input(shape=(1,1)), LSTM(28), Dense(1)])\nbuild_and_train(lstm, \"LSTM\")\n\nbilstm = Sequential([Input(shape=(1,1)), Bidirectional(LSTM(28)), Dense(1)])\nbuild_and_train(bilstm, \"BiLSTM\")\n\ngru = Sequential([Input(shape=(1,1)), GRU(28), Dense(1)])\nbuild_and_train(gru, \"GRU\")\n\nbigru = Sequential([Input(shape=(1,1)), Bidirectional(GRU(28)), Dense(1)])\nbuild_and_train(bigru, \"BiGRU\")\n\n\nprint(\"\\nModel Performance (MSE & R2):\")\nprint(pd.DataFrame(results).T)\n\n\n","metadata":{"ExecuteTime":{"end_time":"2025-10-06T18:46:33.938074Z","start_time":"2025-10-06T18:45:40.731597Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:15:48.209663Z","iopub.execute_input":"2025-10-30T13:15:48.210016Z","iopub.status.idle":"2025-10-30T13:16:46.436725Z","shell.execute_reply.started":"2025-10-30T13:15:48.209991Z","shell.execute_reply":"2025-10-30T13:16:46.435181Z"}},"outputs":[{"name":"stdout","text":"Sample Data:\n    Hours  Scores\n0    1.2      18\n1    2.0      28\n2    3.5      33\n3    4.0      42\n4    5.3      57\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642ms/step\n\nModel Performance (MSE & R2):\n                              MSE        R2\nLinear Regression        5.874146  0.991248\nSVM                    922.643022 -0.374643\nRandom Forest          109.817344  0.836384\nXGBoost                330.158344  0.508098\nKNN                    419.296875  0.375291\nNaive Bayes (approx)   170.000000  0.746718\nMLP (Sklearn)            8.277894  0.987667\nRNN                   3710.210857 -4.527831\nLSTM                  4116.265323 -5.132810\nBiLSTM                3864.072093 -4.757068\nGRU                   3863.803679 -4.756668\nBiGRU                 3601.488976 -4.365846\n","output_type":"stream"}],"execution_count":2},{"id":"22ba9fd86d486a8b","cell_type":"code","source":"\n","metadata":{},"outputs":[],"execution_count":null},{"id":"73c9b3fc-d817-43ac-88bb-03c76bece2f2","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"id":"a22d83a9-b4fb-40c3-8b74-995359f992f8","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}